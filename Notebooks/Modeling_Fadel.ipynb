{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import imblearn\n",
    "import pickle\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('dataraning_new_data.csv', delimiter=',') # upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Resp</th>\n",
       "      <th>VL.t0</th>\n",
       "      <th>CD4.t0</th>\n",
       "      <th>rtlength</th>\n",
       "      <th>pr_A</th>\n",
       "      <th>pr_C</th>\n",
       "      <th>pr_G</th>\n",
       "      <th>pr_R</th>\n",
       "      <th>pr_T</th>\n",
       "      <th>pr_Y</th>\n",
       "      <th>PR_GC</th>\n",
       "      <th>RT_A</th>\n",
       "      <th>RT_C</th>\n",
       "      <th>RT_G</th>\n",
       "      <th>RT_R</th>\n",
       "      <th>RT_T</th>\n",
       "      <th>RT_Y</th>\n",
       "      <th>RT_GC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>145</td>\n",
       "      <td>1005</td>\n",
       "      <td>104</td>\n",
       "      <td>51</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>0.402730</td>\n",
       "      <td>402</td>\n",
       "      <td>167</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.378134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>224</td>\n",
       "      <td>909</td>\n",
       "      <td>110</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>355</td>\n",
       "      <td>151</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>203</td>\n",
       "      <td>0.381375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1017</td>\n",
       "      <td>903</td>\n",
       "      <td>105</td>\n",
       "      <td>47</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>0.389078</td>\n",
       "      <td>360</td>\n",
       "      <td>146</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>201</td>\n",
       "      <td>0.368243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>206</td>\n",
       "      <td>1455</td>\n",
       "      <td>105</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>586</td>\n",
       "      <td>245</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "      <td>0.378527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>572</td>\n",
       "      <td>903</td>\n",
       "      <td>105</td>\n",
       "      <td>50</td>\n",
       "      <td>69</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400673</td>\n",
       "      <td>353</td>\n",
       "      <td>150</td>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Resp  VL.t0  CD4.t0  rtlength  pr_A  pr_C  pr_G  pr_R  pr_T  \\\n",
       "0           1     0    4.3     145      1005   104    51    67     2    71   \n",
       "1           2     0    3.6     224       909   110    49    65    73     0   \n",
       "2           3     0    3.2    1017       903   105    47    67     2    74   \n",
       "3           4     0    5.7     206      1455   105    49    71     1    71   \n",
       "4           5     0    3.5     572       903   105    50    69    73     0   \n",
       "\n",
       "   pr_Y     PR_GC  RT_A  RT_C  RT_G  RT_R  RT_T  RT_Y     RT_GC  \n",
       "0     2  0.402730   402   167   210     1     1     1  0.378134  \n",
       "1     0  0.383838   355   151   193     1     3   203  0.381375  \n",
       "2     2  0.389078   360   146   181     1     7   201  0.368243  \n",
       "3     0  0.405405   586   245   305     1     1   317  0.378527  \n",
       "4     0  0.400673   353   150   184     2     5     1  0.374439  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head() #Visualization of the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff498ea1828>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAADuCAYAAAD7nKGzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data[\"Resp\"].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Resp</th>\n",
       "      <th>VL.t0</th>\n",
       "      <th>CD4.t0</th>\n",
       "      <th>rtlength</th>\n",
       "      <th>pr_A</th>\n",
       "      <th>pr_C</th>\n",
       "      <th>pr_G</th>\n",
       "      <th>pr_R</th>\n",
       "      <th>pr_T</th>\n",
       "      <th>pr_Y</th>\n",
       "      <th>PR_GC</th>\n",
       "      <th>RT_A</th>\n",
       "      <th>RT_C</th>\n",
       "      <th>RT_G</th>\n",
       "      <th>RT_R</th>\n",
       "      <th>RT_T</th>\n",
       "      <th>RT_Y</th>\n",
       "      <th>RT_GC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316746</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.038613</td>\n",
       "      <td>0.060402</td>\n",
       "      <td>-0.289413</td>\n",
       "      <td>-0.104817</td>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.156757</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.073843</td>\n",
       "      <td>0.039735</td>\n",
       "      <td>0.055431</td>\n",
       "      <td>0.039726</td>\n",
       "      <td>0.045710</td>\n",
       "      <td>-0.018059</td>\n",
       "      <td>-0.105165</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.019547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resp</th>\n",
       "      <td>0.316746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363947</td>\n",
       "      <td>-0.127548</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>-0.121435</td>\n",
       "      <td>-0.080273</td>\n",
       "      <td>-0.030747</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>0.315671</td>\n",
       "      <td>0.294487</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>-0.045041</td>\n",
       "      <td>-0.118343</td>\n",
       "      <td>0.070142</td>\n",
       "      <td>0.105843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VL.t0</th>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.363947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.427281</td>\n",
       "      <td>0.327529</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>-0.099714</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>0.018848</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>0.324758</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>-0.080765</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>0.060104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD4.t0</th>\n",
       "      <td>0.038613</td>\n",
       "      <td>-0.127548</td>\n",
       "      <td>-0.427281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.297203</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>0.044449</td>\n",
       "      <td>0.041038</td>\n",
       "      <td>0.086117</td>\n",
       "      <td>-0.024116</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>0.060032</td>\n",
       "      <td>-0.292327</td>\n",
       "      <td>-0.275972</td>\n",
       "      <td>-0.269580</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>-0.054134</td>\n",
       "      <td>-0.101756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rtlength</th>\n",
       "      <td>0.060402</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>0.327529</td>\n",
       "      <td>-0.297203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>-0.129542</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.098628</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.938903</td>\n",
       "      <td>0.944121</td>\n",
       "      <td>-0.090145</td>\n",
       "      <td>-0.076606</td>\n",
       "      <td>0.136990</td>\n",
       "      <td>0.485215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_A</th>\n",
       "      <td>-0.289413</td>\n",
       "      <td>-0.121435</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165435</td>\n",
       "      <td>-0.123698</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.188840</td>\n",
       "      <td>-0.495073</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>-0.023715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_C</th>\n",
       "      <td>-0.104817</td>\n",
       "      <td>-0.080273</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>0.044449</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.165435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>0.062032</td>\n",
       "      <td>-0.024762</td>\n",
       "      <td>-0.053688</td>\n",
       "      <td>0.552580</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_G</th>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.030747</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.041038</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>-0.123698</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>-0.007022</td>\n",
       "      <td>-0.022936</td>\n",
       "      <td>0.350968</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.019386</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>-0.003723</td>\n",
       "      <td>0.028718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_R</th>\n",
       "      <td>-0.156757</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>-0.099714</td>\n",
       "      <td>0.086117</td>\n",
       "      <td>-0.129542</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.062032</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.559403</td>\n",
       "      <td>-0.341439</td>\n",
       "      <td>-0.019549</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>-0.096217</td>\n",
       "      <td>-0.100271</td>\n",
       "      <td>0.188537</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>-0.095437</td>\n",
       "      <td>-0.019180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_T</th>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>-0.024116</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.024762</td>\n",
       "      <td>-0.007022</td>\n",
       "      <td>-0.559403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.459740</td>\n",
       "      <td>-0.054546</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>0.053928</td>\n",
       "      <td>-0.026724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_Y</th>\n",
       "      <td>0.073843</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>0.018848</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>0.098628</td>\n",
       "      <td>-0.188840</td>\n",
       "      <td>-0.053688</td>\n",
       "      <td>-0.022936</td>\n",
       "      <td>-0.341439</td>\n",
       "      <td>-0.459740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>0.089328</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>-0.090885</td>\n",
       "      <td>-0.042089</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.082371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR_GC</th>\n",
       "      <td>0.039735</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>0.060032</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>-0.495073</td>\n",
       "      <td>0.552580</td>\n",
       "      <td>0.350968</td>\n",
       "      <td>-0.019549</td>\n",
       "      <td>-0.054546</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007739</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>-0.081843</td>\n",
       "      <td>0.045487</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>-0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_A</th>\n",
       "      <td>0.055431</td>\n",
       "      <td>0.315671</td>\n",
       "      <td>0.324758</td>\n",
       "      <td>-0.292327</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.089328</td>\n",
       "      <td>-0.007739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938039</td>\n",
       "      <td>0.941122</td>\n",
       "      <td>-0.076158</td>\n",
       "      <td>-0.066181</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.455368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_C</th>\n",
       "      <td>0.039726</td>\n",
       "      <td>0.294487</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>-0.275972</td>\n",
       "      <td>0.938903</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.019386</td>\n",
       "      <td>-0.096217</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>0.938039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>-0.148563</td>\n",
       "      <td>-0.047448</td>\n",
       "      <td>0.147334</td>\n",
       "      <td>0.514687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_G</th>\n",
       "      <td>0.045710</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-0.269580</td>\n",
       "      <td>0.944121</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>-0.100271</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>0.941122</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.167082</td>\n",
       "      <td>-0.050812</td>\n",
       "      <td>0.143727</td>\n",
       "      <td>0.499224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_R</th>\n",
       "      <td>-0.018059</td>\n",
       "      <td>-0.045041</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>-0.090145</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>0.188537</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-0.090885</td>\n",
       "      <td>-0.081843</td>\n",
       "      <td>-0.076158</td>\n",
       "      <td>-0.148563</td>\n",
       "      <td>-0.167082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.210416</td>\n",
       "      <td>-0.233202</td>\n",
       "      <td>-0.085303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_T</th>\n",
       "      <td>-0.105165</td>\n",
       "      <td>-0.118343</td>\n",
       "      <td>-0.080765</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>-0.076606</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>-0.042089</td>\n",
       "      <td>0.045487</td>\n",
       "      <td>-0.066181</td>\n",
       "      <td>-0.047448</td>\n",
       "      <td>-0.050812</td>\n",
       "      <td>-0.210416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.439550</td>\n",
       "      <td>-0.010580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_Y</th>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.070142</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>-0.054134</td>\n",
       "      <td>0.136990</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>-0.003723</td>\n",
       "      <td>-0.095437</td>\n",
       "      <td>0.053928</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.147334</td>\n",
       "      <td>0.143727</td>\n",
       "      <td>-0.233202</td>\n",
       "      <td>-0.439550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_GC</th>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.105843</td>\n",
       "      <td>0.060104</td>\n",
       "      <td>-0.101756</td>\n",
       "      <td>0.485215</td>\n",
       "      <td>-0.023715</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.028718</td>\n",
       "      <td>-0.019180</td>\n",
       "      <td>-0.026724</td>\n",
       "      <td>0.082371</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.455368</td>\n",
       "      <td>0.514687</td>\n",
       "      <td>0.499224</td>\n",
       "      <td>-0.085303</td>\n",
       "      <td>-0.010580</td>\n",
       "      <td>0.103095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0      Resp     VL.t0    CD4.t0  rtlength      pr_A  \\\n",
       "Unnamed: 0    1.000000  0.316746  0.082143  0.038613  0.060402 -0.289413   \n",
       "Resp          0.316746  1.000000  0.363947 -0.127548  0.320095 -0.121435   \n",
       "VL.t0         0.082143  0.363947  1.000000 -0.427281  0.327529 -0.010349   \n",
       "CD4.t0        0.038613 -0.127548 -0.427281  1.000000 -0.297203 -0.022918   \n",
       "rtlength      0.060402  0.320095  0.327529 -0.297203  1.000000  0.004620   \n",
       "pr_A         -0.289413 -0.121435 -0.010349 -0.022918  0.004620  1.000000   \n",
       "pr_C         -0.104817 -0.080273 -0.011644  0.044449  0.001332  0.165435   \n",
       "pr_G         -0.052061 -0.030747  0.012650  0.041038  0.018774 -0.123698   \n",
       "pr_R         -0.156757 -0.106768 -0.099714  0.086117 -0.129542  0.243491   \n",
       "pr_T          0.037121  0.046432  0.063036 -0.024116  0.045435  0.000056   \n",
       "pr_Y          0.073843  0.054466  0.018848 -0.044339  0.098628 -0.188840   \n",
       "PR_GC         0.039735 -0.029655 -0.040033  0.060032 -0.010285 -0.495073   \n",
       "RT_A          0.055431  0.315671  0.324758 -0.292327  0.997939  0.011044   \n",
       "RT_C          0.039726  0.294487  0.299423 -0.275972  0.938903  0.009148   \n",
       "RT_G          0.045710  0.269169  0.300049 -0.269580  0.944121  0.011599   \n",
       "RT_R         -0.018059 -0.045041 -0.085253  0.063620 -0.090145  0.097611   \n",
       "RT_T         -0.105165 -0.118343 -0.080765  0.058562 -0.076606  0.022571   \n",
       "RT_Y          0.024748  0.070142  0.082630 -0.054134  0.136990 -0.043869   \n",
       "RT_GC         0.019547  0.105843  0.060104 -0.101756  0.485215 -0.023715   \n",
       "\n",
       "                pr_C      pr_G      pr_R      pr_T      pr_Y     PR_GC  \\\n",
       "Unnamed: 0 -0.104817 -0.052061 -0.156757  0.037121  0.073843  0.039735   \n",
       "Resp       -0.080273 -0.030747 -0.106768  0.046432  0.054466 -0.029655   \n",
       "VL.t0      -0.011644  0.012650 -0.099714  0.063036  0.018848 -0.040033   \n",
       "CD4.t0      0.044449  0.041038  0.086117 -0.024116 -0.044339  0.060032   \n",
       "rtlength    0.001332  0.018774 -0.129542  0.045435  0.098628 -0.010285   \n",
       "pr_A        0.165435 -0.123698  0.243491  0.000056 -0.188840 -0.495073   \n",
       "pr_C        1.000000  0.158018  0.062032 -0.024762 -0.053688  0.552580   \n",
       "pr_G        0.158018  1.000000 -0.024814 -0.007022 -0.022936  0.350968   \n",
       "pr_R        0.062032 -0.024814  1.000000 -0.559403 -0.341439 -0.019549   \n",
       "pr_T       -0.024762 -0.007022 -0.559403  1.000000 -0.459740 -0.054546   \n",
       "pr_Y       -0.053688 -0.022936 -0.341439 -0.459740  1.000000  0.056307   \n",
       "PR_GC       0.552580  0.350968 -0.019549 -0.054546  0.056307  1.000000   \n",
       "RT_A        0.006332  0.020450 -0.121479  0.047112  0.089328 -0.007739   \n",
       "RT_C        0.015336  0.019386 -0.096217  0.058270  0.057745  0.004325   \n",
       "RT_G        0.010107  0.014290 -0.100271  0.028524  0.087273 -0.014258   \n",
       "RT_R       -0.063966  0.031384  0.188537 -0.082375 -0.090885 -0.081843   \n",
       "RT_T        0.050509 -0.006384  0.088849 -0.050419 -0.042089  0.045487   \n",
       "RT_Y       -0.014506 -0.003723 -0.095437  0.053928  0.027596  0.004808   \n",
       "RT_GC       0.011730  0.028718 -0.019180 -0.026724  0.082371 -0.000513   \n",
       "\n",
       "                RT_A      RT_C      RT_G      RT_R      RT_T      RT_Y  \\\n",
       "Unnamed: 0  0.055431  0.039726  0.045710 -0.018059 -0.105165  0.024748   \n",
       "Resp        0.315671  0.294487  0.269169 -0.045041 -0.118343  0.070142   \n",
       "VL.t0       0.324758  0.299423  0.300049 -0.085253 -0.080765  0.082630   \n",
       "CD4.t0     -0.292327 -0.275972 -0.269580  0.063620  0.058562 -0.054134   \n",
       "rtlength    0.997939  0.938903  0.944121 -0.090145 -0.076606  0.136990   \n",
       "pr_A        0.011044  0.009148  0.011599  0.097611  0.022571 -0.043869   \n",
       "pr_C        0.006332  0.015336  0.010107 -0.063966  0.050509 -0.014506   \n",
       "pr_G        0.020450  0.019386  0.014290  0.031384 -0.006384 -0.003723   \n",
       "pr_R       -0.121479 -0.096217 -0.100271  0.188537  0.088849 -0.095437   \n",
       "pr_T        0.047112  0.058270  0.028524 -0.082375 -0.050419  0.053928   \n",
       "pr_Y        0.089328  0.057745  0.087273 -0.090885 -0.042089  0.027596   \n",
       "PR_GC      -0.007739  0.004325 -0.014258 -0.081843  0.045487  0.004808   \n",
       "RT_A        1.000000  0.938039  0.941122 -0.076158 -0.066181  0.133181   \n",
       "RT_C        0.938039  1.000000  0.907075 -0.148563 -0.047448  0.147334   \n",
       "RT_G        0.941122  0.907075  1.000000 -0.167082 -0.050812  0.143727   \n",
       "RT_R       -0.076158 -0.148563 -0.167082  1.000000 -0.210416 -0.233202   \n",
       "RT_T       -0.066181 -0.047448 -0.050812 -0.210416  1.000000 -0.439550   \n",
       "RT_Y        0.133181  0.147334  0.143727 -0.233202 -0.439550  1.000000   \n",
       "RT_GC       0.455368  0.514687  0.499224 -0.085303 -0.010580  0.103095   \n",
       "\n",
       "               RT_GC  \n",
       "Unnamed: 0  0.019547  \n",
       "Resp        0.105843  \n",
       "VL.t0       0.060104  \n",
       "CD4.t0     -0.101756  \n",
       "rtlength    0.485215  \n",
       "pr_A       -0.023715  \n",
       "pr_C        0.011730  \n",
       "pr_G        0.028718  \n",
       "pr_R       -0.019180  \n",
       "pr_T       -0.026724  \n",
       "pr_Y        0.082371  \n",
       "PR_GC      -0.000513  \n",
       "RT_A        0.455368  \n",
       "RT_C        0.514687  \n",
       "RT_G        0.499224  \n",
       "RT_R       -0.085303  \n",
       "RT_T       -0.010580  \n",
       "RT_Y        0.103095  \n",
       "RT_GC       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how correlated are the different features\n",
    "new_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at the missing data\n",
    "new_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:  0    733\n",
      "1    187\n",
      "Name: Resp, dtype: int64\n",
      "\n",
      "Number of 1 in the new  dataset:  733\n"
     ]
    }
   ],
   "source": [
    "#Our dataset is imbalanced, lets fix that\n",
    "VL = new_data['VL.t0'].values.tolist()   # get all the values of viral load in a list\n",
    "CD4 = new_data['CD4.t0'].values.tolist() # get all the values of CD4+ cells in a list\n",
    "rt_length = new_data['rtlength'].values.tolist()\n",
    "pr_A = new_data['pr_A'].values.tolist()\n",
    "pr_C = new_data['pr_C'].values.tolist()\n",
    "pr_G = new_data['pr_G'].values.tolist()\n",
    "pr_R = new_data['pr_R'].values.tolist()\n",
    "pr_T = new_data['pr_T'].values.tolist()\n",
    "pr_Y = new_data['pr_Y'].values.tolist()\n",
    "pr_GC = new_data['PR_GC'].values.tolist()\n",
    "rt_A = new_data['RT_A'].values.tolist()\n",
    "rt_C = new_data['RT_C'].values.tolist()\n",
    "rt_G = new_data['RT_G'].values.tolist()\n",
    "rt_R = new_data['RT_R'].values.tolist()\n",
    "rt_T = new_data['RT_T'].values.tolist()\n",
    "rt_Y = new_data['RT_Y'].values.tolist()\n",
    "rt_GC = new_data['RT_GC'].values.tolist()\n",
    "\n",
    "inp = list(zip(VL,CD4,pr_A,pr_C,pr_G,pr_R,pr_T,pr_Y,pr_GC,rt_length,rt_A,rt_C,rt_G,rt_R,rt_T,rt_Y,rt_GC))  # create a new input feature with the two numerical features\n",
    "lab = new_data[\"Resp\"].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = imblearn.over_sampling.SMOTE(sampling_strategy = 'auto', kind = 'regular',random_state=5)\n",
    "\n",
    "\n",
    "inputs,label = sm.fit_sample(new_data[['VL.t0' ,'CD4.t0' ,'rtlength' ,'pr_A' ,'pr_C' ,'pr_G' ,'pr_R' ,'pr_T' ,'pr_Y' ,'PR_GC','RT_A' ,'RT_C' ,'RT_G' ,'RT_R' ,'RT_T','RT_Y','RT_GC'\n",
    "]],new_data['Resp'])\n",
    "print(\"Original dataset: \",new_data['Resp'].value_counts())\n",
    "\n",
    "\n",
    "compt = 0\n",
    "for i in range(len(label)):\n",
    "    if label[i]==1:\n",
    "        compt += 1\n",
    "print(\"\\nNumber of 1 in the new  dataset: \",compt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(90)\n",
    "input_train, input_test, label_input, label_test = train_test_split(inputs, label,random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- k-Nearest Neighbors (k-NN) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k different averages: [0.7816567860604557, 0.7725736161515978, 0.7689287619562848, 0.7653334986362509, 0.7617051747326977, 0.7543411551668433, 0.7562168173177348, 0.7506796204043911, 0.7379020054249412, 0.7387945660422724, 0.7232893777847906, 0.718760754081855, 0.7123960657905611, 0.7033134969832218, 0.7043052393511109, 0.6978503106943472, 0.6933210858898933, 0.6924033541464734, 0.6951143971327457, 0.6969495600688261, 0.6914281420703439, 0.6860146969321281, 0.684146623687908, 0.6868822366987505, 0.6932710441884754, 0.6951474577162651, 0.6896512108438715, 0.6851221363147968, 0.6869321281247888, 0.6806423521102422, 0.6851800674736455, 0.6842791665727447, 0.6824524941956135, 0.6816011841699916, 0.6870725604670559, 0.6833864556800336, 0.6824940453380821]\n",
      "\n",
      "Max average, index of Max: 0.7816567860604557 || 0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "# search for an optimal value of K for KNN with cross validation\n",
    "# range of k we want to try\n",
    "k_range = range(2, 39)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, input_train, label_input, cv = 10, scoring='accuracy')\n",
    "    #scores = cross_val_score(knn, inputs, label, cv = 10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "print(\"k different averages:\", k_scores)\n",
    "print(\"\\nMax average, index of Max:\", max(k_scores),\"||\", k_scores.index(max(k_scores)))\n",
    "pos = k_scores.index(max(k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cross-validated accuracy')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxicrFsYYY3yyYmGMMcan/wfofL+1ob1cSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot how accuracy changes as we vary k\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.8826205641492265\n",
      "\n",
      "The accuracy score that we get is:  0.7220708446866485\n",
      "\n",
      " Confusion Matrix:  [[111  66]\n",
      " [ 36 154]]\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(3)\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "model = model.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",model.score(input_train, label_input)) \n",
    "pred = model.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)  \n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.63      0.69       177\n",
      "           1       0.70      0.81      0.75       190\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       367\n",
      "   macro avg       0.73      0.72      0.72       367\n",
      "weighted avg       0.73      0.72      0.72       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Multi Layer Perceptron (MLP) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.6424021838034577\n",
      "\n",
      "The accuracy score that we get is:  0.6430517711171662\n",
      "\n",
      " Confusion Matrix:  [[ 61 116]\n",
      " [ 15 175]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,activation='relu',\n",
    "                    hidden_layer_sizes=(950,950,950,2),random_state =5)\n",
    "\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "clf = clf.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",clf.score(input_train, label_input)) #evaluating the training error\n",
    "pred = clf.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.34      0.48       177\n",
      "           1       0.60      0.92      0.73       190\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       367\n",
      "   macro avg       0.70      0.63      0.60       367\n",
      "weighted avg       0.70      0.64      0.61       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.7443130118289354\n",
      "\n",
      "The accuracy score that we get is:  0.7193460490463215\n",
      "\n",
      " Confusion Matrix:  [[135  42]\n",
      " [ 61 129]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awa/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#with cross validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "\n",
    "print(\"\\n\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "logReg = logReg.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",logReg.score(input_train, label_input)) #evaluating the training error\n",
    "pred = logReg.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72       177\n",
      "           1       0.75      0.68      0.71       190\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       367\n",
      "   macro avg       0.72      0.72      0.72       367\n",
      "weighted avg       0.72      0.72      0.72       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 - SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.6469517743403094\n",
      "\n",
      "The accuracy score that we get is:  0.659400544959128\n",
      "\n",
      " Confusion Matrix:  [[ 98  79]\n",
      " [ 46 144]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awa/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgdc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=25,random_state=5)\n",
    "sgdc = sgdc.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",sgdc.score(input_train, label_input)) #evaluating the training error\n",
    "pred = sgdc.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.55      0.61       177\n",
      "           1       0.65      0.76      0.70       190\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       367\n",
      "   macro avg       0.66      0.66      0.65       367\n",
      "weighted avg       0.66      0.66      0.66       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  1.0\n",
      "\n",
      "The accuracy score that we get is:  0.782016348773842\n",
      "\n",
      " Confusion Matrix:  [[137  40]\n",
      " [ 40 150]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(random_state=5)\n",
    "dt = dt.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",dt.score(input_train, label_input)) #evaluating the training error\n",
    "pred = dt.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       177\n",
      "           1       0.79      0.79      0.79       190\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       367\n",
      "   macro avg       0.78      0.78      0.78       367\n",
      "weighted avg       0.78      0.78      0.78       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.9426751592356688\n",
      "\n",
      "The accuracy score that we get is:  0.8119891008174387\n",
      "\n",
      " Confusion Matrix:  [[143  34]\n",
      " [ 35 155]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state=5)\n",
    "GBC = GBC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",GBC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = GBC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       177\n",
      "           1       0.82      0.82      0.82       190\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       367\n",
      "   macro avg       0.81      0.81      0.81       367\n",
      "weighted avg       0.81      0.81      0.81       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.8644222020018199\n",
      "\n",
      "The accuracy score that we get is:  0.8310626702997275\n",
      "\n",
      " Confusion Matrix:  [[144  33]\n",
      " [ 29 161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ABC = AdaBoostClassifier(random_state=5)\n",
    "ABC = ABC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",ABC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = ABC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DummyClassifier is a classifier that makes predictions using simple rules.\n",
    "This classifier is useful as a simple baseline to compare with other\n",
    "(real) classifiers. Do not use it for real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.4986351228389445\n",
      "\n",
      "The accuracy score that we get is:  0.4904632152588556\n",
      "\n",
      " Confusion Matrix:  [[85 92]\n",
      " [95 95]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier(strategy=\"uniform\",random_state=5)\n",
    "dc = dc.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",dc.score(input_train, label_input)) #evaluating the training error\n",
    "pred = dc.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.48      0.48       177\n",
      "           1       0.51      0.50      0.50       190\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       367\n",
      "   macro avg       0.49      0.49      0.49       367\n",
      "weighted avg       0.49      0.49      0.49       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.9827115559599636\n",
      "\n",
      "The accuracy score that we get is:  0.8310626702997275\n",
      "\n",
      " Confusion Matrix:  [[150  27]\n",
      " [ 35 155]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awa/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(max_depth=11, random_state=5)\n",
    "RF = RF.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",RF.score(input_train, label_input)) #evaluating the training error\n",
    "pred = RF.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       177\n",
      "           1       0.85      0.82      0.83       190\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       367\n",
      "   macro avg       0.83      0.83      0.83       367\n",
      "weighted avg       0.83      0.83      0.83       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- Extra trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  1.0\n",
      "\n",
      "The accuracy score that we get is:  0.8719346049046321\n",
      "\n",
      " Confusion Matrix:  [[153  24]\n",
      " [ 23 167]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awa/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ETC = ExtraTreesClassifier(random_state=5)\n",
    "ETC = ETC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",ETC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = ETC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87       177\n",
      "           1       0.87      0.88      0.88       190\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       367\n",
      "   macro avg       0.87      0.87      0.87       367\n",
      "weighted avg       0.87      0.87      0.87       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
