{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import imblearn\n",
    "import pickle\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('./all/dataraning_new_data.csv') # upload the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Resp</th>\n",
       "      <th>VL.t0</th>\n",
       "      <th>CD4.t0</th>\n",
       "      <th>rtlength</th>\n",
       "      <th>pr_A</th>\n",
       "      <th>pr_C</th>\n",
       "      <th>pr_G</th>\n",
       "      <th>pr_R</th>\n",
       "      <th>pr_T</th>\n",
       "      <th>pr_Y</th>\n",
       "      <th>PR_GC</th>\n",
       "      <th>RT_A</th>\n",
       "      <th>RT_C</th>\n",
       "      <th>RT_G</th>\n",
       "      <th>RT_R</th>\n",
       "      <th>RT_T</th>\n",
       "      <th>RT_Y</th>\n",
       "      <th>RT_GC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>145</td>\n",
       "      <td>1005</td>\n",
       "      <td>104</td>\n",
       "      <td>51</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>0.402730</td>\n",
       "      <td>402</td>\n",
       "      <td>167</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.378134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>224</td>\n",
       "      <td>909</td>\n",
       "      <td>110</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>355</td>\n",
       "      <td>151</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>203</td>\n",
       "      <td>0.381375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1017</td>\n",
       "      <td>903</td>\n",
       "      <td>105</td>\n",
       "      <td>47</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>0.389078</td>\n",
       "      <td>360</td>\n",
       "      <td>146</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>201</td>\n",
       "      <td>0.368243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>206</td>\n",
       "      <td>1455</td>\n",
       "      <td>105</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>586</td>\n",
       "      <td>245</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "      <td>0.378527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>572</td>\n",
       "      <td>903</td>\n",
       "      <td>105</td>\n",
       "      <td>50</td>\n",
       "      <td>69</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400673</td>\n",
       "      <td>353</td>\n",
       "      <td>150</td>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Resp  VL.t0  CD4.t0  rtlength  pr_A  pr_C  pr_G  pr_R  pr_T  \\\n",
       "0           1     0    4.3     145      1005   104    51    67     2    71   \n",
       "1           2     0    3.6     224       909   110    49    65    73     0   \n",
       "2           3     0    3.2    1017       903   105    47    67     2    74   \n",
       "3           4     0    5.7     206      1455   105    49    71     1    71   \n",
       "4           5     0    3.5     572       903   105    50    69    73     0   \n",
       "\n",
       "   pr_Y     PR_GC  RT_A  RT_C  RT_G  RT_R  RT_T  RT_Y     RT_GC  \n",
       "0     2  0.402730   402   167   210     1     1     1  0.378134  \n",
       "1     0  0.383838   355   151   193     1     3   203  0.381375  \n",
       "2     2  0.389078   360   146   181     1     7   201  0.368243  \n",
       "3     0  0.405405   586   245   305     1     1   317  0.378527  \n",
       "4     0  0.400673   353   150   184     2     5     1  0.374439  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f66bdc22e48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADuCAYAAAAZZe3jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66bdc7c710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data[\"Resp\"].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Resp</th>\n",
       "      <th>VL.t0</th>\n",
       "      <th>CD4.t0</th>\n",
       "      <th>rtlength</th>\n",
       "      <th>pr_A</th>\n",
       "      <th>pr_C</th>\n",
       "      <th>pr_G</th>\n",
       "      <th>pr_R</th>\n",
       "      <th>pr_T</th>\n",
       "      <th>pr_Y</th>\n",
       "      <th>PR_GC</th>\n",
       "      <th>RT_A</th>\n",
       "      <th>RT_C</th>\n",
       "      <th>RT_G</th>\n",
       "      <th>RT_R</th>\n",
       "      <th>RT_T</th>\n",
       "      <th>RT_Y</th>\n",
       "      <th>RT_GC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316746</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.038613</td>\n",
       "      <td>0.060402</td>\n",
       "      <td>-0.289413</td>\n",
       "      <td>-0.104817</td>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.156757</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.073843</td>\n",
       "      <td>0.039735</td>\n",
       "      <td>0.055431</td>\n",
       "      <td>0.039726</td>\n",
       "      <td>0.045710</td>\n",
       "      <td>-0.018059</td>\n",
       "      <td>-0.105165</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.019547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resp</th>\n",
       "      <td>0.316746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363947</td>\n",
       "      <td>-0.127548</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>-0.121435</td>\n",
       "      <td>-0.080273</td>\n",
       "      <td>-0.030747</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>0.315671</td>\n",
       "      <td>0.294487</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>-0.045041</td>\n",
       "      <td>-0.118343</td>\n",
       "      <td>0.070142</td>\n",
       "      <td>0.105843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VL.t0</th>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.363947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.427281</td>\n",
       "      <td>0.327529</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>-0.099714</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>0.018848</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>0.324758</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>-0.080765</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>0.060104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD4.t0</th>\n",
       "      <td>0.038613</td>\n",
       "      <td>-0.127548</td>\n",
       "      <td>-0.427281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.297203</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>0.044449</td>\n",
       "      <td>0.041038</td>\n",
       "      <td>0.086117</td>\n",
       "      <td>-0.024116</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>0.060032</td>\n",
       "      <td>-0.292327</td>\n",
       "      <td>-0.275972</td>\n",
       "      <td>-0.269580</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>-0.054134</td>\n",
       "      <td>-0.101756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rtlength</th>\n",
       "      <td>0.060402</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>0.327529</td>\n",
       "      <td>-0.297203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>-0.129542</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.098628</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.938903</td>\n",
       "      <td>0.944121</td>\n",
       "      <td>-0.090145</td>\n",
       "      <td>-0.076606</td>\n",
       "      <td>0.136990</td>\n",
       "      <td>0.485215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_A</th>\n",
       "      <td>-0.289413</td>\n",
       "      <td>-0.121435</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165435</td>\n",
       "      <td>-0.123698</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.188840</td>\n",
       "      <td>-0.495073</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>-0.023715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_C</th>\n",
       "      <td>-0.104817</td>\n",
       "      <td>-0.080273</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>0.044449</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.165435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>0.062032</td>\n",
       "      <td>-0.024762</td>\n",
       "      <td>-0.053688</td>\n",
       "      <td>0.552580</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_G</th>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.030747</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.041038</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>-0.123698</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>-0.007022</td>\n",
       "      <td>-0.022936</td>\n",
       "      <td>0.350968</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.019386</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>-0.003723</td>\n",
       "      <td>0.028718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_R</th>\n",
       "      <td>-0.156757</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>-0.099714</td>\n",
       "      <td>0.086117</td>\n",
       "      <td>-0.129542</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.062032</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.559403</td>\n",
       "      <td>-0.341439</td>\n",
       "      <td>-0.019549</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>-0.096217</td>\n",
       "      <td>-0.100271</td>\n",
       "      <td>0.188537</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>-0.095437</td>\n",
       "      <td>-0.019180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_T</th>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>-0.024116</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.024762</td>\n",
       "      <td>-0.007022</td>\n",
       "      <td>-0.559403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.459740</td>\n",
       "      <td>-0.054546</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>0.053928</td>\n",
       "      <td>-0.026724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_Y</th>\n",
       "      <td>0.073843</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>0.018848</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>0.098628</td>\n",
       "      <td>-0.188840</td>\n",
       "      <td>-0.053688</td>\n",
       "      <td>-0.022936</td>\n",
       "      <td>-0.341439</td>\n",
       "      <td>-0.459740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>0.089328</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>-0.090885</td>\n",
       "      <td>-0.042089</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.082371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR_GC</th>\n",
       "      <td>0.039735</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>0.060032</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>-0.495073</td>\n",
       "      <td>0.552580</td>\n",
       "      <td>0.350968</td>\n",
       "      <td>-0.019549</td>\n",
       "      <td>-0.054546</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007739</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>-0.081843</td>\n",
       "      <td>0.045487</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>-0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_A</th>\n",
       "      <td>0.055431</td>\n",
       "      <td>0.315671</td>\n",
       "      <td>0.324758</td>\n",
       "      <td>-0.292327</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.089328</td>\n",
       "      <td>-0.007739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938039</td>\n",
       "      <td>0.941122</td>\n",
       "      <td>-0.076158</td>\n",
       "      <td>-0.066181</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.455368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_C</th>\n",
       "      <td>0.039726</td>\n",
       "      <td>0.294487</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>-0.275972</td>\n",
       "      <td>0.938903</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.019386</td>\n",
       "      <td>-0.096217</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>0.938039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>-0.148563</td>\n",
       "      <td>-0.047448</td>\n",
       "      <td>0.147334</td>\n",
       "      <td>0.514687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_G</th>\n",
       "      <td>0.045710</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-0.269580</td>\n",
       "      <td>0.944121</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>-0.100271</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>0.941122</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.167082</td>\n",
       "      <td>-0.050812</td>\n",
       "      <td>0.143727</td>\n",
       "      <td>0.499224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_R</th>\n",
       "      <td>-0.018059</td>\n",
       "      <td>-0.045041</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>-0.090145</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>0.188537</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-0.090885</td>\n",
       "      <td>-0.081843</td>\n",
       "      <td>-0.076158</td>\n",
       "      <td>-0.148563</td>\n",
       "      <td>-0.167082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.210416</td>\n",
       "      <td>-0.233202</td>\n",
       "      <td>-0.085303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_T</th>\n",
       "      <td>-0.105165</td>\n",
       "      <td>-0.118343</td>\n",
       "      <td>-0.080765</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>-0.076606</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>-0.042089</td>\n",
       "      <td>0.045487</td>\n",
       "      <td>-0.066181</td>\n",
       "      <td>-0.047448</td>\n",
       "      <td>-0.050812</td>\n",
       "      <td>-0.210416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.439550</td>\n",
       "      <td>-0.010580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_Y</th>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.070142</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>-0.054134</td>\n",
       "      <td>0.136990</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>-0.003723</td>\n",
       "      <td>-0.095437</td>\n",
       "      <td>0.053928</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.147334</td>\n",
       "      <td>0.143727</td>\n",
       "      <td>-0.233202</td>\n",
       "      <td>-0.439550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_GC</th>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.105843</td>\n",
       "      <td>0.060104</td>\n",
       "      <td>-0.101756</td>\n",
       "      <td>0.485215</td>\n",
       "      <td>-0.023715</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.028718</td>\n",
       "      <td>-0.019180</td>\n",
       "      <td>-0.026724</td>\n",
       "      <td>0.082371</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.455368</td>\n",
       "      <td>0.514687</td>\n",
       "      <td>0.499224</td>\n",
       "      <td>-0.085303</td>\n",
       "      <td>-0.010580</td>\n",
       "      <td>0.103095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0      Resp     VL.t0    CD4.t0  rtlength      pr_A  \\\n",
       "Unnamed: 0    1.000000  0.316746  0.082143  0.038613  0.060402 -0.289413   \n",
       "Resp          0.316746  1.000000  0.363947 -0.127548  0.320095 -0.121435   \n",
       "VL.t0         0.082143  0.363947  1.000000 -0.427281  0.327529 -0.010349   \n",
       "CD4.t0        0.038613 -0.127548 -0.427281  1.000000 -0.297203 -0.022918   \n",
       "rtlength      0.060402  0.320095  0.327529 -0.297203  1.000000  0.004620   \n",
       "pr_A         -0.289413 -0.121435 -0.010349 -0.022918  0.004620  1.000000   \n",
       "pr_C         -0.104817 -0.080273 -0.011644  0.044449  0.001332  0.165435   \n",
       "pr_G         -0.052061 -0.030747  0.012650  0.041038  0.018774 -0.123698   \n",
       "pr_R         -0.156757 -0.106768 -0.099714  0.086117 -0.129542  0.243491   \n",
       "pr_T          0.037121  0.046432  0.063036 -0.024116  0.045435  0.000056   \n",
       "pr_Y          0.073843  0.054466  0.018848 -0.044339  0.098628 -0.188840   \n",
       "PR_GC         0.039735 -0.029655 -0.040033  0.060032 -0.010285 -0.495073   \n",
       "RT_A          0.055431  0.315671  0.324758 -0.292327  0.997939  0.011044   \n",
       "RT_C          0.039726  0.294487  0.299423 -0.275972  0.938903  0.009148   \n",
       "RT_G          0.045710  0.269169  0.300049 -0.269580  0.944121  0.011599   \n",
       "RT_R         -0.018059 -0.045041 -0.085253  0.063620 -0.090145  0.097611   \n",
       "RT_T         -0.105165 -0.118343 -0.080765  0.058562 -0.076606  0.022571   \n",
       "RT_Y          0.024748  0.070142  0.082630 -0.054134  0.136990 -0.043869   \n",
       "RT_GC         0.019547  0.105843  0.060104 -0.101756  0.485215 -0.023715   \n",
       "\n",
       "                pr_C      pr_G      pr_R      pr_T      pr_Y     PR_GC  \\\n",
       "Unnamed: 0 -0.104817 -0.052061 -0.156757  0.037121  0.073843  0.039735   \n",
       "Resp       -0.080273 -0.030747 -0.106768  0.046432  0.054466 -0.029655   \n",
       "VL.t0      -0.011644  0.012650 -0.099714  0.063036  0.018848 -0.040033   \n",
       "CD4.t0      0.044449  0.041038  0.086117 -0.024116 -0.044339  0.060032   \n",
       "rtlength    0.001332  0.018774 -0.129542  0.045435  0.098628 -0.010285   \n",
       "pr_A        0.165435 -0.123698  0.243491  0.000056 -0.188840 -0.495073   \n",
       "pr_C        1.000000  0.158018  0.062032 -0.024762 -0.053688  0.552580   \n",
       "pr_G        0.158018  1.000000 -0.024814 -0.007022 -0.022936  0.350968   \n",
       "pr_R        0.062032 -0.024814  1.000000 -0.559403 -0.341439 -0.019549   \n",
       "pr_T       -0.024762 -0.007022 -0.559403  1.000000 -0.459740 -0.054546   \n",
       "pr_Y       -0.053688 -0.022936 -0.341439 -0.459740  1.000000  0.056307   \n",
       "PR_GC       0.552580  0.350968 -0.019549 -0.054546  0.056307  1.000000   \n",
       "RT_A        0.006332  0.020450 -0.121479  0.047112  0.089328 -0.007739   \n",
       "RT_C        0.015336  0.019386 -0.096217  0.058270  0.057745  0.004325   \n",
       "RT_G        0.010107  0.014290 -0.100271  0.028524  0.087273 -0.014258   \n",
       "RT_R       -0.063966  0.031384  0.188537 -0.082375 -0.090885 -0.081843   \n",
       "RT_T        0.050509 -0.006384  0.088849 -0.050419 -0.042089  0.045487   \n",
       "RT_Y       -0.014506 -0.003723 -0.095437  0.053928  0.027596  0.004808   \n",
       "RT_GC       0.011730  0.028718 -0.019180 -0.026724  0.082371 -0.000513   \n",
       "\n",
       "                RT_A      RT_C      RT_G      RT_R      RT_T      RT_Y  \\\n",
       "Unnamed: 0  0.055431  0.039726  0.045710 -0.018059 -0.105165  0.024748   \n",
       "Resp        0.315671  0.294487  0.269169 -0.045041 -0.118343  0.070142   \n",
       "VL.t0       0.324758  0.299423  0.300049 -0.085253 -0.080765  0.082630   \n",
       "CD4.t0     -0.292327 -0.275972 -0.269580  0.063620  0.058562 -0.054134   \n",
       "rtlength    0.997939  0.938903  0.944121 -0.090145 -0.076606  0.136990   \n",
       "pr_A        0.011044  0.009148  0.011599  0.097611  0.022571 -0.043869   \n",
       "pr_C        0.006332  0.015336  0.010107 -0.063966  0.050509 -0.014506   \n",
       "pr_G        0.020450  0.019386  0.014290  0.031384 -0.006384 -0.003723   \n",
       "pr_R       -0.121479 -0.096217 -0.100271  0.188537  0.088849 -0.095437   \n",
       "pr_T        0.047112  0.058270  0.028524 -0.082375 -0.050419  0.053928   \n",
       "pr_Y        0.089328  0.057745  0.087273 -0.090885 -0.042089  0.027596   \n",
       "PR_GC      -0.007739  0.004325 -0.014258 -0.081843  0.045487  0.004808   \n",
       "RT_A        1.000000  0.938039  0.941122 -0.076158 -0.066181  0.133181   \n",
       "RT_C        0.938039  1.000000  0.907075 -0.148563 -0.047448  0.147334   \n",
       "RT_G        0.941122  0.907075  1.000000 -0.167082 -0.050812  0.143727   \n",
       "RT_R       -0.076158 -0.148563 -0.167082  1.000000 -0.210416 -0.233202   \n",
       "RT_T       -0.066181 -0.047448 -0.050812 -0.210416  1.000000 -0.439550   \n",
       "RT_Y        0.133181  0.147334  0.143727 -0.233202 -0.439550  1.000000   \n",
       "RT_GC       0.455368  0.514687  0.499224 -0.085303 -0.010580  0.103095   \n",
       "\n",
       "               RT_GC  \n",
       "Unnamed: 0  0.019547  \n",
       "Resp        0.105843  \n",
       "VL.t0       0.060104  \n",
       "CD4.t0     -0.101756  \n",
       "rtlength    0.485215  \n",
       "pr_A       -0.023715  \n",
       "pr_C        0.011730  \n",
       "pr_G        0.028718  \n",
       "pr_R       -0.019180  \n",
       "pr_T       -0.026724  \n",
       "pr_Y        0.082371  \n",
       "PR_GC      -0.000513  \n",
       "RT_A        0.455368  \n",
       "RT_C        0.514687  \n",
       "RT_G        0.499224  \n",
       "RT_R       -0.085303  \n",
       "RT_T       -0.010580  \n",
       "RT_Y        0.103095  \n",
       "RT_GC       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how correlated are the different features\n",
    "new_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at the missing data\n",
    "new_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:  0    733\n",
      "1    187\n",
      "Name: Resp, dtype: int64\n",
      "\n",
      "Number of 1 in the new  dataset:  733\n"
     ]
    }
   ],
   "source": [
    "#Our dataset is imbalanced, lets fix that\n",
    "VL = new_data['VL.t0'].values.tolist()   # get all the values of viral load in a list\n",
    "CD4 = new_data['CD4.t0'].values.tolist() # get all the values of CD4+ cells in a list\n",
    "rt_length = new_data['rtlength'].values.tolist()\n",
    "pr_A = new_data['pr_A'].values.tolist()\n",
    "pr_C = new_data['pr_C'].values.tolist()\n",
    "pr_G = new_data['pr_G'].values.tolist()\n",
    "pr_R = new_data['pr_R'].values.tolist()\n",
    "pr_T = new_data['pr_T'].values.tolist()\n",
    "pr_Y = new_data['pr_Y'].values.tolist()\n",
    "pr_GC = new_data['PR_GC'].values.tolist()\n",
    "rt_A = new_data['RT_A'].values.tolist()\n",
    "rt_C = new_data['RT_C'].values.tolist()\n",
    "rt_G = new_data['RT_G'].values.tolist()\n",
    "rt_R = new_data['RT_R'].values.tolist()\n",
    "rt_T = new_data['RT_T'].values.tolist()\n",
    "rt_Y = new_data['RT_Y'].values.tolist()\n",
    "rt_GC = new_data['RT_GC'].values.tolist()\n",
    "\n",
    "inp = list(zip(VL,CD4,pr_A,pr_C,pr_G,pr_R,pr_T,pr_Y,pr_GC,rt_length,rt_A,rt_C,rt_G,rt_R,rt_T,rt_Y,rt_GC))  # create a new input feature with the two numerical features\n",
    "lab = new_data[\"Resp\"].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = imblearn.over_sampling.SMOTE(sampling_strategy = 'auto', kind = 'regular',random_state=5)\n",
    "\n",
    "#VL.t0 ,CD4.t0 ,rtlength ,pr_A ,pr_C ,pr_G ,pr_R ,pr_T ,pr_Y ,PR_GC ,RT_A ,RT_C ,RT_G ,RT_R ,RT_T,RT_Y,RT_GC\n",
    "\n",
    "inputs,label = sm.fit_sample(new_data[['VL.t0' ,'CD4.t0' ,'rtlength' ,'pr_A' ,'pr_C' ,'pr_G' ,'pr_R' ,'pr_T' ,'pr_Y' ,'PR_GC','RT_A' ,'RT_C' ,'RT_G' ,'RT_R' ,'RT_T','RT_Y','RT_GC'\n",
    "]],new_data['Resp'])\n",
    "print(\"Original dataset: \",new_data['Resp'].value_counts())\n",
    "\n",
    "#the missing datas Are  deleted. Why?\n",
    "\n",
    "compt = 0\n",
    "for i in range(len(label)):\n",
    "    if label[i]==1:\n",
    "        compt += 1\n",
    "print(\"\\nNumber of 1 in the new  dataset: \",compt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(90)\n",
    "input_train, input_test, label_input, label_test = train_test_split(inputs, label,random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- k-Nearest Neighbors (k-NN) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k different averages: [0.7816567860604557, 0.7725736161515978, 0.7689287619562848, 0.7653334986362509, 0.7617051747326977, 0.7543411551668433, 0.7562168173177348, 0.7506796204043911, 0.7379020054249412, 0.7387945660422724, 0.7232893777847906, 0.718760754081855, 0.7123960657905611, 0.7033134969832218, 0.7043052393511109, 0.6978503106943472, 0.6933210858898933, 0.6924033541464734, 0.6951143971327457, 0.6969495600688261, 0.6914281420703439, 0.6860146969321281, 0.684146623687908, 0.6868822366987505, 0.6932710441884754, 0.6951474577162651, 0.6896512108438715, 0.6851221363147968, 0.6869321281247888, 0.6806423521102422, 0.6851800674736455, 0.6842791665727447, 0.6824524941956135, 0.6816011841699916, 0.6870725604670559, 0.6833864556800336, 0.6824940453380821]\n",
      "\n",
      "Max average, index of Max: 0.7816567860604557 || 0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "# search for an optimal value of K for KNN with cross validation\n",
    "# range of k we want to try\n",
    "k_range = range(2, 39)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, input_train, label_input, cv = 10, scoring='accuracy')\n",
    "    #scores = cross_val_score(knn, inputs, label, cv = 10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "print(\"k different averages:\", k_scores)\n",
    "print(\"\\nMax average, index of Max:\", max(k_scores),\"||\", k_scores.index(max(k_scores)))\n",
    "pos = k_scores.index(max(k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f66b6e88a20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvFrgbeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66bdbe2278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot how accuracy changes as we vary k\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "# plt.plot(x_axis, y_axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.8826205641492265\n",
      "\n",
      "The accuracy score that we get is:  0.7220708446866485\n",
      "\n",
      " Confusion Matrix:  [[111  66]\n",
      " [ 36 154]]\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(3)\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "model = model.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",model.score(input_train, label_input)) \n",
    "pred = model.predict(input_test)\n",
    "#score = metrics.accuracy_score(pred, label_test)\n",
    "score = model.score(input_test, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)  \n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Multi Layer Perceptron (MLP) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.6424021838034577\n",
      "\n",
      "The accuracy score that we get is:  0.6430517711171662\n",
      "\n",
      " Confusion Matrix:  [[ 61 116]\n",
      " [ 15 175]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,activation='relu',\n",
    "                    hidden_layer_sizes=(950,950,950,2),random_state =5)\n",
    "\n",
    "#print(\"\\t%%%%% The following results are the different accuracies that we get at each step of the CV %%%%%\")\n",
    "#acc = cross_val_score(clf, input_train, label_input, cv=10, scoring='accuracy')\n",
    "#print(\"\\nAccuracies: \",acc)\n",
    "#print(\"\\nMean accuracy: \", acc.mean() )\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "clf = clf.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",clf.score(input_train, label_input)) #evaluating the training error\n",
    "pred = clf.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.7443130118289354\n",
      "\n",
      "The accuracy score that we get is:  0.7193460490463215\n",
      "\n",
      " Confusion Matrix:  [[135  42]\n",
      " [ 61 129]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#with cross validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "#print(\"\\t%%%%% The following results are the different accuracy that we get at each step of the CV %%%%%\")\n",
    "#acc = cross_val_score(logReg, input_train, label_input, cv=10, scoring='accuracy')\n",
    "#print(\"\\nAccuracies: \",acc)\n",
    "#print(\"\\nMean accuracy: \", acc.mean() )\n",
    "\n",
    "print(\"\\n\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "logReg = logReg.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",logReg.score(input_train, label_input)) #evaluating the training error\n",
    "pred = logReg.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 - SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.6469517743403094\n",
      "\n",
      "The accuracy score that we get is:  0.659400544959128\n",
      "\n",
      " Confusion Matrix:  [[ 98  79]\n",
      " [ 46 144]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgdc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=25,random_state=5)\n",
    "sgdc = sgdc.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",sgdc.score(input_train, label_input)) #evaluating the training error\n",
    "pred = sgdc.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  1.0\n",
      "\n",
      "The accuracy score that we get is:  0.782016348773842\n",
      "\n",
      " Confusion Matrix:  [[137  40]\n",
      " [ 40 150]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(random_state=5)\n",
    "dt = dt.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",dt.score(input_train, label_input)) #evaluating the training error\n",
    "pred = dt.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.9426751592356688\n",
      "\n",
      "The accuracy score that we get is:  0.8119891008174387\n",
      "\n",
      " Confusion Matrix:  [[143  34]\n",
      " [ 35 155]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state=5)\n",
    "GBC = GBC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",GBC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = GBC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.8644222020018199\n",
      "\n",
      "The accuracy score that we get is:  0.8310626702997275\n",
      "\n",
      " Confusion Matrix:  [[144  33]\n",
      " [ 29 161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ABC = AdaBoostClassifier(random_state=5)\n",
    "ABC = ABC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",ABC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = ABC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AdaBoostClassifier??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DummyClassifier is a classifier that makes predictions using simple rules.\n",
    "This classifier is useful as a simple baseline to compare with other\n",
    "(real) classifiers. Do not use it for real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.4986351228389445\n",
      "\n",
      "The accuracy score that we get is:  0.4904632152588556\n",
      "\n",
      " Confusion Matrix:  [[85 92]\n",
      " [95 95]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier(strategy=\"uniform\",random_state=5)\n",
    "dc = dc.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",dc.score(input_train, label_input)) #evaluating the training error\n",
    "pred = dc.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.9827115559599636\n",
      "\n",
      "The accuracy score that we get is:  0.8310626702997275\n",
      "\n",
      " Confusion Matrix:  [[150  27]\n",
      " [ 35 155]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(max_depth=11, random_state=5)\n",
    "RF = RF.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",RF.score(input_train, label_input)) #evaluating the training error\n",
    "pred = RF.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- Extra trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  1.0\n",
      "\n",
      "The accuracy score that we get is:  0.8719346049046321\n",
      "\n",
      " Confusion Matrix:  [[153  24]\n",
      " [ 23 167]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ETC = ExtraTreesClassifier(random_state=5)\n",
    "ETC = ETC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",ETC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = ETC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
