{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import imblearn\n",
    "import pickle\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./all/dataraning_new_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cb1cf2c9e4f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./all/dataraning_new_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# upload the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./all/dataraning_new_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "new_data = pd.read_csv('./all/dataraning_new_data.csv') # upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_data.head() #Visualization of the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data[\"Resp\"].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets see how correlated are the different features\n",
    "new_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at the missing data\n",
    "new_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Our dataset is imbalanced, lets fix that\n",
    "VL = new_data['VL.t0'].values.tolist()   # get all the values of viral load in a list\n",
    "CD4 = new_data['CD4.t0'].values.tolist() # get all the values of CD4+ cells in a list\n",
    "rt_length = new_data['rtlength'].values.tolist()\n",
    "pr_A = new_data['pr_A'].values.tolist()\n",
    "pr_C = new_data['pr_C'].values.tolist()\n",
    "pr_G = new_data['pr_G'].values.tolist()\n",
    "pr_R = new_data['pr_R'].values.tolist()\n",
    "pr_T = new_data['pr_T'].values.tolist()\n",
    "pr_Y = new_data['pr_Y'].values.tolist()\n",
    "pr_GC = new_data['PR_GC'].values.tolist()\n",
    "rt_A = new_data['RT_A'].values.tolist()\n",
    "rt_C = new_data['RT_C'].values.tolist()\n",
    "rt_G = new_data['RT_G'].values.tolist()\n",
    "rt_R = new_data['RT_R'].values.tolist()\n",
    "rt_T = new_data['RT_T'].values.tolist()\n",
    "rt_Y = new_data['RT_Y'].values.tolist()\n",
    "rt_GC = new_data['RT_GC'].values.tolist()\n",
    "\n",
    "inp = list(zip(VL,CD4,pr_A,pr_C,pr_G,pr_R,pr_T,pr_Y,pr_GC,rt_length,rt_A,rt_C,rt_G,rt_R,rt_T,rt_Y,rt_GC))  # create a new input feature with the two numerical features\n",
    "lab = new_data[\"Resp\"].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = imblearn.over_sampling.SMOTE(sampling_strategy = 'auto', kind = 'regular',random_state=5)\n",
    "\n",
    "\n",
    "inputs,label = sm.fit_sample(new_data[['VL.t0' ,'CD4.t0' ,'rtlength' ,'pr_A' ,'pr_C' ,'pr_G' ,'pr_R' ,'pr_T' ,'pr_Y' ,'PR_GC','RT_A' ,'RT_C' ,'RT_G' ,'RT_R' ,'RT_T','RT_Y','RT_GC'\n",
    "]],new_data['Resp'])\n",
    "print(\"Original dataset: \",new_data['Resp'].value_counts())\n",
    "\n",
    "\n",
    "compt = 0\n",
    "for i in range(len(label)):\n",
    "    if label[i]==1:\n",
    "        compt += 1\n",
    "print(\"\\nNumber of 1 in the new  dataset: \",compt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(90)\n",
    "input_train, input_test, label_input, label_test = train_test_split(inputs, label,random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- k-Nearest Neighbors (k-NN) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "# search for an optimal value of K for KNN with cross validation\n",
    "# range of k we want to try\n",
    "k_range = range(2, 39)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, input_train, label_input, cv = 10, scoring='accuracy')\n",
    "    #scores = cross_val_score(knn, inputs, label, cv = 10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "print(\"k different averages:\", k_scores)\n",
    "print(\"\\nMax average, index of Max:\", max(k_scores),\"||\", k_scores.index(max(k_scores)))\n",
    "pos = k_scores.index(max(k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot how accuracy changes as we vary k\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(3)\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "model = model.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",model.score(input_train, label_input)) \n",
    "pred = model.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)  \n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Multi Layer Perceptron (MLP) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,activation='relu',\n",
    "                    hidden_layer_sizes=(950,950,950,2),random_state =5)\n",
    "\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "clf = clf.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",clf.score(input_train, label_input)) #evaluating the training error\n",
    "pred = clf.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with cross validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "\n",
    "print(\"\\n\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "logReg = logReg.fit(input_train,label_input)\n",
    "print(\"\\n Training score: \",logReg.score(input_train, label_input)) #evaluating the training error\n",
    "pred = logReg.predict(input_test)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 - SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgdc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=25,random_state=5)\n",
    "sgdc = sgdc.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",sgdc.score(input_train, label_input)) #evaluating the training error\n",
    "pred = sgdc.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(random_state=5)\n",
    "dt = dt.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",dt.score(input_train, label_input)) #evaluating the training error\n",
    "pred = dt.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state=5)\n",
    "GBC = GBC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",GBC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = GBC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ABC = AdaBoostClassifier(random_state=5)\n",
    "ABC = ABC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",ABC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = ABC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DummyClassifier is a classifier that makes predictions using simple rules.\n",
    "This classifier is useful as a simple baseline to compare with other\n",
    "(real) classifiers. Do not use it for real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier(strategy=\"uniform\",random_state=5)\n",
    "dc = dc.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",dc.score(input_train, label_input)) #evaluating the training error\n",
    "pred = dc.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(max_depth=11, random_state=5)\n",
    "RF = RF.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",RF.score(input_train, label_input)) #evaluating the training error\n",
    "pred = RF.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- Extra trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ETC = ExtraTreesClassifier(random_state=5)\n",
    "ETC = ETC.fit(input_train, label_input)\n",
    "print(\"\\n Training score: \",ETC.score(input_train, label_input)) #evaluating the training error\n",
    "pred = ETC.predict(input_test)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
