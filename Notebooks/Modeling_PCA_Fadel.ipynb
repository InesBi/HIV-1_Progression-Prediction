{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import imblearn\n",
    "import pickle\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import linalg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('./all/dataraning_new_data.csv') # upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Resp</th>\n",
       "      <th>VL.t0</th>\n",
       "      <th>CD4.t0</th>\n",
       "      <th>rtlength</th>\n",
       "      <th>pr_A</th>\n",
       "      <th>pr_C</th>\n",
       "      <th>pr_G</th>\n",
       "      <th>pr_R</th>\n",
       "      <th>pr_T</th>\n",
       "      <th>pr_Y</th>\n",
       "      <th>PR_GC</th>\n",
       "      <th>RT_A</th>\n",
       "      <th>RT_C</th>\n",
       "      <th>RT_G</th>\n",
       "      <th>RT_R</th>\n",
       "      <th>RT_T</th>\n",
       "      <th>RT_Y</th>\n",
       "      <th>RT_GC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>145</td>\n",
       "      <td>1005</td>\n",
       "      <td>104</td>\n",
       "      <td>51</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>0.402730</td>\n",
       "      <td>402</td>\n",
       "      <td>167</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.378134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>224</td>\n",
       "      <td>909</td>\n",
       "      <td>110</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>355</td>\n",
       "      <td>151</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>203</td>\n",
       "      <td>0.381375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1017</td>\n",
       "      <td>903</td>\n",
       "      <td>105</td>\n",
       "      <td>47</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>0.389078</td>\n",
       "      <td>360</td>\n",
       "      <td>146</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>201</td>\n",
       "      <td>0.368243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>206</td>\n",
       "      <td>1455</td>\n",
       "      <td>105</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>586</td>\n",
       "      <td>245</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "      <td>0.378527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>572</td>\n",
       "      <td>903</td>\n",
       "      <td>105</td>\n",
       "      <td>50</td>\n",
       "      <td>69</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400673</td>\n",
       "      <td>353</td>\n",
       "      <td>150</td>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Resp  VL.t0  CD4.t0  rtlength  pr_A  pr_C  pr_G  pr_R  pr_T  \\\n",
       "0           1     0    4.3     145      1005   104    51    67     2    71   \n",
       "1           2     0    3.6     224       909   110    49    65    73     0   \n",
       "2           3     0    3.2    1017       903   105    47    67     2    74   \n",
       "3           4     0    5.7     206      1455   105    49    71     1    71   \n",
       "4           5     0    3.5     572       903   105    50    69    73     0   \n",
       "\n",
       "   pr_Y     PR_GC  RT_A  RT_C  RT_G  RT_R  RT_T  RT_Y     RT_GC  \n",
       "0     2  0.402730   402   167   210     1     1     1  0.378134  \n",
       "1     0  0.383838   355   151   193     1     3   203  0.381375  \n",
       "2     2  0.389078   360   146   181     1     7   201  0.368243  \n",
       "3     0  0.405405   586   245   305     1     1   317  0.378527  \n",
       "4     0  0.400673   353   150   184     2     5     1  0.374439  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6ee8582048>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADuCAYAAAAZZe3jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f7072fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data[\"Resp\"].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Resp</th>\n",
       "      <th>VL.t0</th>\n",
       "      <th>CD4.t0</th>\n",
       "      <th>rtlength</th>\n",
       "      <th>pr_A</th>\n",
       "      <th>pr_C</th>\n",
       "      <th>pr_G</th>\n",
       "      <th>pr_R</th>\n",
       "      <th>pr_T</th>\n",
       "      <th>pr_Y</th>\n",
       "      <th>PR_GC</th>\n",
       "      <th>RT_A</th>\n",
       "      <th>RT_C</th>\n",
       "      <th>RT_G</th>\n",
       "      <th>RT_R</th>\n",
       "      <th>RT_T</th>\n",
       "      <th>RT_Y</th>\n",
       "      <th>RT_GC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316746</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.038613</td>\n",
       "      <td>0.060402</td>\n",
       "      <td>-0.289413</td>\n",
       "      <td>-0.104817</td>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.156757</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.073843</td>\n",
       "      <td>0.039735</td>\n",
       "      <td>0.055431</td>\n",
       "      <td>0.039726</td>\n",
       "      <td>0.045710</td>\n",
       "      <td>-0.018059</td>\n",
       "      <td>-0.105165</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.019547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resp</th>\n",
       "      <td>0.316746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363947</td>\n",
       "      <td>-0.127548</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>-0.121435</td>\n",
       "      <td>-0.080273</td>\n",
       "      <td>-0.030747</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>0.315671</td>\n",
       "      <td>0.294487</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>-0.045041</td>\n",
       "      <td>-0.118343</td>\n",
       "      <td>0.070142</td>\n",
       "      <td>0.105843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VL.t0</th>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.363947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.427281</td>\n",
       "      <td>0.327529</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>-0.099714</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>0.018848</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>0.324758</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>-0.080765</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>0.060104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD4.t0</th>\n",
       "      <td>0.038613</td>\n",
       "      <td>-0.127548</td>\n",
       "      <td>-0.427281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.297203</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>0.044449</td>\n",
       "      <td>0.041038</td>\n",
       "      <td>0.086117</td>\n",
       "      <td>-0.024116</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>0.060032</td>\n",
       "      <td>-0.292327</td>\n",
       "      <td>-0.275972</td>\n",
       "      <td>-0.269580</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>-0.054134</td>\n",
       "      <td>-0.101756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rtlength</th>\n",
       "      <td>0.060402</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>0.327529</td>\n",
       "      <td>-0.297203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>-0.129542</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.098628</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.938903</td>\n",
       "      <td>0.944121</td>\n",
       "      <td>-0.090145</td>\n",
       "      <td>-0.076606</td>\n",
       "      <td>0.136990</td>\n",
       "      <td>0.485215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_A</th>\n",
       "      <td>-0.289413</td>\n",
       "      <td>-0.121435</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165435</td>\n",
       "      <td>-0.123698</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.188840</td>\n",
       "      <td>-0.495073</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>-0.023715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_C</th>\n",
       "      <td>-0.104817</td>\n",
       "      <td>-0.080273</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>0.044449</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.165435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>0.062032</td>\n",
       "      <td>-0.024762</td>\n",
       "      <td>-0.053688</td>\n",
       "      <td>0.552580</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_G</th>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.030747</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.041038</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>-0.123698</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>-0.007022</td>\n",
       "      <td>-0.022936</td>\n",
       "      <td>0.350968</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.019386</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>-0.003723</td>\n",
       "      <td>0.028718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_R</th>\n",
       "      <td>-0.156757</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>-0.099714</td>\n",
       "      <td>0.086117</td>\n",
       "      <td>-0.129542</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.062032</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.559403</td>\n",
       "      <td>-0.341439</td>\n",
       "      <td>-0.019549</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>-0.096217</td>\n",
       "      <td>-0.100271</td>\n",
       "      <td>0.188537</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>-0.095437</td>\n",
       "      <td>-0.019180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_T</th>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0.063036</td>\n",
       "      <td>-0.024116</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.024762</td>\n",
       "      <td>-0.007022</td>\n",
       "      <td>-0.559403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.459740</td>\n",
       "      <td>-0.054546</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>0.053928</td>\n",
       "      <td>-0.026724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_Y</th>\n",
       "      <td>0.073843</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>0.018848</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>0.098628</td>\n",
       "      <td>-0.188840</td>\n",
       "      <td>-0.053688</td>\n",
       "      <td>-0.022936</td>\n",
       "      <td>-0.341439</td>\n",
       "      <td>-0.459740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>0.089328</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>-0.090885</td>\n",
       "      <td>-0.042089</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.082371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR_GC</th>\n",
       "      <td>0.039735</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>0.060032</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>-0.495073</td>\n",
       "      <td>0.552580</td>\n",
       "      <td>0.350968</td>\n",
       "      <td>-0.019549</td>\n",
       "      <td>-0.054546</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007739</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>-0.081843</td>\n",
       "      <td>0.045487</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>-0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_A</th>\n",
       "      <td>0.055431</td>\n",
       "      <td>0.315671</td>\n",
       "      <td>0.324758</td>\n",
       "      <td>-0.292327</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.089328</td>\n",
       "      <td>-0.007739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938039</td>\n",
       "      <td>0.941122</td>\n",
       "      <td>-0.076158</td>\n",
       "      <td>-0.066181</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.455368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_C</th>\n",
       "      <td>0.039726</td>\n",
       "      <td>0.294487</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>-0.275972</td>\n",
       "      <td>0.938903</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.019386</td>\n",
       "      <td>-0.096217</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>0.938039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>-0.148563</td>\n",
       "      <td>-0.047448</td>\n",
       "      <td>0.147334</td>\n",
       "      <td>0.514687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_G</th>\n",
       "      <td>0.045710</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-0.269580</td>\n",
       "      <td>0.944121</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>-0.100271</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>0.941122</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.167082</td>\n",
       "      <td>-0.050812</td>\n",
       "      <td>0.143727</td>\n",
       "      <td>0.499224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_R</th>\n",
       "      <td>-0.018059</td>\n",
       "      <td>-0.045041</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>-0.090145</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>0.188537</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-0.090885</td>\n",
       "      <td>-0.081843</td>\n",
       "      <td>-0.076158</td>\n",
       "      <td>-0.148563</td>\n",
       "      <td>-0.167082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.210416</td>\n",
       "      <td>-0.233202</td>\n",
       "      <td>-0.085303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_T</th>\n",
       "      <td>-0.105165</td>\n",
       "      <td>-0.118343</td>\n",
       "      <td>-0.080765</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>-0.076606</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>-0.042089</td>\n",
       "      <td>0.045487</td>\n",
       "      <td>-0.066181</td>\n",
       "      <td>-0.047448</td>\n",
       "      <td>-0.050812</td>\n",
       "      <td>-0.210416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.439550</td>\n",
       "      <td>-0.010580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_Y</th>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.070142</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>-0.054134</td>\n",
       "      <td>0.136990</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>-0.003723</td>\n",
       "      <td>-0.095437</td>\n",
       "      <td>0.053928</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.147334</td>\n",
       "      <td>0.143727</td>\n",
       "      <td>-0.233202</td>\n",
       "      <td>-0.439550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT_GC</th>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.105843</td>\n",
       "      <td>0.060104</td>\n",
       "      <td>-0.101756</td>\n",
       "      <td>0.485215</td>\n",
       "      <td>-0.023715</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.028718</td>\n",
       "      <td>-0.019180</td>\n",
       "      <td>-0.026724</td>\n",
       "      <td>0.082371</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.455368</td>\n",
       "      <td>0.514687</td>\n",
       "      <td>0.499224</td>\n",
       "      <td>-0.085303</td>\n",
       "      <td>-0.010580</td>\n",
       "      <td>0.103095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0      Resp     VL.t0    CD4.t0  rtlength      pr_A  \\\n",
       "Unnamed: 0    1.000000  0.316746  0.082143  0.038613  0.060402 -0.289413   \n",
       "Resp          0.316746  1.000000  0.363947 -0.127548  0.320095 -0.121435   \n",
       "VL.t0         0.082143  0.363947  1.000000 -0.427281  0.327529 -0.010349   \n",
       "CD4.t0        0.038613 -0.127548 -0.427281  1.000000 -0.297203 -0.022918   \n",
       "rtlength      0.060402  0.320095  0.327529 -0.297203  1.000000  0.004620   \n",
       "pr_A         -0.289413 -0.121435 -0.010349 -0.022918  0.004620  1.000000   \n",
       "pr_C         -0.104817 -0.080273 -0.011644  0.044449  0.001332  0.165435   \n",
       "pr_G         -0.052061 -0.030747  0.012650  0.041038  0.018774 -0.123698   \n",
       "pr_R         -0.156757 -0.106768 -0.099714  0.086117 -0.129542  0.243491   \n",
       "pr_T          0.037121  0.046432  0.063036 -0.024116  0.045435  0.000056   \n",
       "pr_Y          0.073843  0.054466  0.018848 -0.044339  0.098628 -0.188840   \n",
       "PR_GC         0.039735 -0.029655 -0.040033  0.060032 -0.010285 -0.495073   \n",
       "RT_A          0.055431  0.315671  0.324758 -0.292327  0.997939  0.011044   \n",
       "RT_C          0.039726  0.294487  0.299423 -0.275972  0.938903  0.009148   \n",
       "RT_G          0.045710  0.269169  0.300049 -0.269580  0.944121  0.011599   \n",
       "RT_R         -0.018059 -0.045041 -0.085253  0.063620 -0.090145  0.097611   \n",
       "RT_T         -0.105165 -0.118343 -0.080765  0.058562 -0.076606  0.022571   \n",
       "RT_Y          0.024748  0.070142  0.082630 -0.054134  0.136990 -0.043869   \n",
       "RT_GC         0.019547  0.105843  0.060104 -0.101756  0.485215 -0.023715   \n",
       "\n",
       "                pr_C      pr_G      pr_R      pr_T      pr_Y     PR_GC  \\\n",
       "Unnamed: 0 -0.104817 -0.052061 -0.156757  0.037121  0.073843  0.039735   \n",
       "Resp       -0.080273 -0.030747 -0.106768  0.046432  0.054466 -0.029655   \n",
       "VL.t0      -0.011644  0.012650 -0.099714  0.063036  0.018848 -0.040033   \n",
       "CD4.t0      0.044449  0.041038  0.086117 -0.024116 -0.044339  0.060032   \n",
       "rtlength    0.001332  0.018774 -0.129542  0.045435  0.098628 -0.010285   \n",
       "pr_A        0.165435 -0.123698  0.243491  0.000056 -0.188840 -0.495073   \n",
       "pr_C        1.000000  0.158018  0.062032 -0.024762 -0.053688  0.552580   \n",
       "pr_G        0.158018  1.000000 -0.024814 -0.007022 -0.022936  0.350968   \n",
       "pr_R        0.062032 -0.024814  1.000000 -0.559403 -0.341439 -0.019549   \n",
       "pr_T       -0.024762 -0.007022 -0.559403  1.000000 -0.459740 -0.054546   \n",
       "pr_Y       -0.053688 -0.022936 -0.341439 -0.459740  1.000000  0.056307   \n",
       "PR_GC       0.552580  0.350968 -0.019549 -0.054546  0.056307  1.000000   \n",
       "RT_A        0.006332  0.020450 -0.121479  0.047112  0.089328 -0.007739   \n",
       "RT_C        0.015336  0.019386 -0.096217  0.058270  0.057745  0.004325   \n",
       "RT_G        0.010107  0.014290 -0.100271  0.028524  0.087273 -0.014258   \n",
       "RT_R       -0.063966  0.031384  0.188537 -0.082375 -0.090885 -0.081843   \n",
       "RT_T        0.050509 -0.006384  0.088849 -0.050419 -0.042089  0.045487   \n",
       "RT_Y       -0.014506 -0.003723 -0.095437  0.053928  0.027596  0.004808   \n",
       "RT_GC       0.011730  0.028718 -0.019180 -0.026724  0.082371 -0.000513   \n",
       "\n",
       "                RT_A      RT_C      RT_G      RT_R      RT_T      RT_Y  \\\n",
       "Unnamed: 0  0.055431  0.039726  0.045710 -0.018059 -0.105165  0.024748   \n",
       "Resp        0.315671  0.294487  0.269169 -0.045041 -0.118343  0.070142   \n",
       "VL.t0       0.324758  0.299423  0.300049 -0.085253 -0.080765  0.082630   \n",
       "CD4.t0     -0.292327 -0.275972 -0.269580  0.063620  0.058562 -0.054134   \n",
       "rtlength    0.997939  0.938903  0.944121 -0.090145 -0.076606  0.136990   \n",
       "pr_A        0.011044  0.009148  0.011599  0.097611  0.022571 -0.043869   \n",
       "pr_C        0.006332  0.015336  0.010107 -0.063966  0.050509 -0.014506   \n",
       "pr_G        0.020450  0.019386  0.014290  0.031384 -0.006384 -0.003723   \n",
       "pr_R       -0.121479 -0.096217 -0.100271  0.188537  0.088849 -0.095437   \n",
       "pr_T        0.047112  0.058270  0.028524 -0.082375 -0.050419  0.053928   \n",
       "pr_Y        0.089328  0.057745  0.087273 -0.090885 -0.042089  0.027596   \n",
       "PR_GC      -0.007739  0.004325 -0.014258 -0.081843  0.045487  0.004808   \n",
       "RT_A        1.000000  0.938039  0.941122 -0.076158 -0.066181  0.133181   \n",
       "RT_C        0.938039  1.000000  0.907075 -0.148563 -0.047448  0.147334   \n",
       "RT_G        0.941122  0.907075  1.000000 -0.167082 -0.050812  0.143727   \n",
       "RT_R       -0.076158 -0.148563 -0.167082  1.000000 -0.210416 -0.233202   \n",
       "RT_T       -0.066181 -0.047448 -0.050812 -0.210416  1.000000 -0.439550   \n",
       "RT_Y        0.133181  0.147334  0.143727 -0.233202 -0.439550  1.000000   \n",
       "RT_GC       0.455368  0.514687  0.499224 -0.085303 -0.010580  0.103095   \n",
       "\n",
       "               RT_GC  \n",
       "Unnamed: 0  0.019547  \n",
       "Resp        0.105843  \n",
       "VL.t0       0.060104  \n",
       "CD4.t0     -0.101756  \n",
       "rtlength    0.485215  \n",
       "pr_A       -0.023715  \n",
       "pr_C        0.011730  \n",
       "pr_G        0.028718  \n",
       "pr_R       -0.019180  \n",
       "pr_T       -0.026724  \n",
       "pr_Y        0.082371  \n",
       "PR_GC      -0.000513  \n",
       "RT_A        0.455368  \n",
       "RT_C        0.514687  \n",
       "RT_G        0.499224  \n",
       "RT_R       -0.085303  \n",
       "RT_T       -0.010580  \n",
       "RT_Y        0.103095  \n",
       "RT_GC       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how correlated are the different features\n",
    "new_data.corr()\n",
    "#mat_cov = np.array(new_data.cov())\n",
    "#linalg.eigvals(mat_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at the missing data\n",
    "new_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:  0    733\n",
      "1    187\n",
      "Name: Resp, dtype: int64\n",
      "\n",
      "Number of 1 in the new  dataset:  733\n"
     ]
    }
   ],
   "source": [
    "#Our dataset is imbalanced, lets fix that\n",
    "VL = new_data['VL.t0'].values.tolist()   # get all the values of viral load in a list\n",
    "CD4 = new_data['CD4.t0'].values.tolist() # get all the values of CD4+ cells in a list\n",
    "rt_length = new_data['rtlength'].values.tolist()\n",
    "pr_A = new_data['pr_A'].values.tolist()\n",
    "pr_C = new_data['pr_C'].values.tolist()\n",
    "pr_G = new_data['pr_G'].values.tolist()\n",
    "pr_R = new_data['pr_R'].values.tolist()\n",
    "pr_T = new_data['pr_T'].values.tolist()\n",
    "pr_Y = new_data['pr_Y'].values.tolist()\n",
    "pr_GC = new_data['PR_GC'].values.tolist()\n",
    "rt_A = new_data['RT_A'].values.tolist()\n",
    "rt_C = new_data['RT_C'].values.tolist()\n",
    "rt_G = new_data['RT_G'].values.tolist()\n",
    "rt_R = new_data['RT_R'].values.tolist()\n",
    "rt_T = new_data['RT_T'].values.tolist()\n",
    "rt_Y = new_data['RT_Y'].values.tolist()\n",
    "rt_GC = new_data['RT_GC'].values.tolist()\n",
    "\n",
    "inp = list(zip(VL,CD4,pr_A,pr_C,pr_G,pr_R,pr_T,pr_Y,pr_GC,rt_length,rt_A,rt_C,rt_G,rt_R,rt_T,rt_Y,rt_GC))  # create a new input feature with the two numerical features\n",
    "lab = new_data[\"Resp\"].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = imblearn.over_sampling.SMOTE(sampling_strategy = 'auto', kind = 'regular',random_state=5)\n",
    "\n",
    "inputs,label = sm.fit_sample(new_data[['VL.t0' ,'CD4.t0' ,'rtlength' ,'pr_A' ,'pr_C' ,'pr_G' ,'pr_R' ,'pr_T' ,'pr_Y' ,'PR_GC','RT_A' ,'RT_C' ,'RT_G' ,'RT_R' ,'RT_T','RT_Y','RT_GC'\n",
    "]],new_data['Resp'])\n",
    "print(\"Original dataset: \",new_data['Resp'].value_counts())\n",
    "\n",
    "\n",
    "compt = 0\n",
    "for i in range(len(label)):\n",
    "    if label[i]==1:\n",
    "        compt += 1\n",
    "print(\"\\nNumber of 1 in the new  dataset: \",compt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(90)\n",
    "input_train, input_test, label_input, label_test = train_test_split(inputs, label,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=12, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_train_PCA = pca.fit_transform(input_train)  # fit the model with input_train and apply dimensionality reduction to input_train \n",
    "input_test_PCA = pca.transform(input_test)       # apply dimensionality reduction to input_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- k-Nearest Neighbors (k-NN) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k different averages: [0.778929513333183, 0.7680031407554343, 0.7689371022398546, 0.7662343995371518, 0.7607877435400371, 0.7543328148832735, 0.754398635499553, 0.7515970515970516, 0.7379020054249412, 0.7369762339487111, 0.7223719465921301, 0.7196698449909459, 0.7133051566996521, 0.7033134969832218, 0.70339614844202, 0.6996684925125292, 0.6933210858898933, 0.6915024532455725, 0.6951143971327457, 0.695114697683505, 0.6896016199685924, 0.6841965151139463, 0.6832291924952475, 0.6859648055060898, 0.6932710441884754, 0.6951557979998346, 0.6896512108438715, 0.6851221363147968, 0.6878495593174494, 0.6824606842038035, 0.6851800674736455, 0.6842791665727447, 0.683369925388274, 0.6825020850708925, 0.6870725604670559, 0.6824938950627024, 0.683403136247173]\n",
      "\n",
      "Max average, index of Max: 0.778929513333183 || 0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "# search for an optimal value of K for KNN with cross validation\n",
    "# range of k we want to try\n",
    "k_range = range(2, 39)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, input_train_PCA, label_input, cv = 10, scoring='accuracy')\n",
    "    #scores = cross_val_score(knn, inputs, label, cv = 10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "print(\"k different averages:\", k_scores)\n",
    "print(\"\\nMax average, index of Max:\", max(k_scores),\"||\", k_scores.index(max(k_scores)))\n",
    "pos = k_scores.index(max(k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7faea559a978>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faeaaef8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot how accuracy changes as we vary k\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.8835304822565969\n",
      "\n",
      "The accuracy score that we get is:  0.7220708446866485\n",
      "\n",
      " Confusion Matrix:  [[110  67]\n",
      " [ 35 155]]\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(3)\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "model = model.fit(input_train_PCA,label_input)\n",
    "print(\"\\n Training score: \",model.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = model.predict(input_test_PCA)\n",
    "#score = metrics.accuracy_score(pred, label_test)\n",
    "score = model.score(input_test_PCA, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)  #evaluating the test error\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Multi Layer Perceptron (MLP) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.9545040946314831\n",
      "\n",
      "The accuracy score that we get is:  0.7983651226158038\n",
      "\n",
      " Confusion Matrix:  [[135  42]\n",
      " [ 32 158]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,activation='relu',\n",
    "                    hidden_layer_sizes=(950,950,950,2),random_state =5)\n",
    "\n",
    "print(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "clf = clf.fit(input_train_PCA,label_input)\n",
    "print(\"\\n Training score: \",clf.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = clf.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t %%% Now lets fit our model first and then we test with our test set %%%\n",
      "\n",
      " Training score:  0.6860782529572339\n",
      "\n",
      "The accuracy score that we get is:  0.6975476839237057\n",
      "\n",
      " Confusion Matrix:  [[145  32]\n",
      " [ 79 111]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#with cross validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "\n",
    "print(\"\\n\\t %%% Now lets fit our model first and then we test with our test set %%%\")\n",
    "logReg = logReg.fit(input_train_PCA,label_input)\n",
    "print(\"\\n Training score: \",logReg.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = logReg.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred, label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 - SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.651501364877161\n",
      "\n",
      "The accuracy score that we get is:  0.6485013623978202\n",
      "\n",
      " Confusion Matrix:  [[154  23]\n",
      " [106  84]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgdc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=25,random_state=5)\n",
    "sgdc = sgdc.fit(input_train_PCA, label_input)\n",
    "print(\"\\n Training score: \",sgdc.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = sgdc.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  1.0\n",
      "\n",
      "The accuracy score that we get is:  0.779291553133515\n",
      "\n",
      " Confusion Matrix:  [[132  45]\n",
      " [ 36 154]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(random_state=5)\n",
    "dt = dt.fit(input_train_PCA, label_input)\n",
    "print(\"\\n Training score: \",dt.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = dt.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.9117379435850773\n",
      "\n",
      "The accuracy score that we get is:  0.776566757493188\n",
      "\n",
      " Confusion Matrix:  [[141  36]\n",
      " [ 46 144]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state=5)\n",
    "GBC = GBC.fit(input_train_PCA, label_input)\n",
    "print(\"\\n Training score: \",GBC.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = GBC.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.7979981801637852\n",
      "\n",
      "The accuracy score that we get is:  0.7275204359673024\n",
      "\n",
      " Confusion Matrix:  [[140  37]\n",
      " [ 63 127]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ABC = AdaBoostClassifier(random_state=5)\n",
    "ABC = ABC.fit(input_train_PCA, label_input)\n",
    "print(\"\\n Training score: \",ABC.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = ABC.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DummyClassifier is a classifier that makes predictions using simple rules.\n",
    "This classifier is useful as a simple baseline to compare with other\n",
    "(real) classifiers. Do not use it for real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.48771610555050043\n",
      "\n",
      "The accuracy score that we get is:  0.4877384196185286\n",
      "\n",
      " Confusion Matrix:  [[88 89]\n",
      " [99 91]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier(random_state=5)\n",
    "dc = dc.fit(input_train_PCA, label_input)\n",
    "print(\"\\n Training score: \",dc.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = dc.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  0.978161965423112\n",
      "\n",
      "The accuracy score that we get is:  0.8310626702997275\n",
      "\n",
      " Confusion Matrix:  [[146  31]\n",
      " [ 31 159]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(max_depth=11, random_state=5)\n",
    "RF = RF.fit(input_train_PCA, label_input)\n",
    "print(\"\\n Training score: \",RF.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = RF.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- Extra trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training score:  1.0\n",
      "\n",
      "The accuracy score that we get is:  0.8365122615803815\n",
      "\n",
      " Confusion Matrix:  [[148  29]\n",
      " [ 31 159]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadel/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ETC = ExtraTreesClassifier(random_state=5)\n",
    "ETC = ETC.fit(input_train_PCA, label_input)\n",
    "print(\"\\n Training score: \",ETC.score(input_train_PCA, label_input)) #evaluating the training error\n",
    "pred = ETC.predict(input_test_PCA)\n",
    "score = metrics.accuracy_score(pred,label_test)\n",
    "print(\"\\nThe accuracy score that we get is: \",score)\n",
    "print(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
